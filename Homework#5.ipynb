{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 5\n",
    "Assignment Group 12  \n",
    "Ke Xu  \n",
    "FangZhou Liu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split the data to training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "df = pd.read_csv('imdb_master.csv')\n",
    "X = df['review']\n",
    "y = df['label']\n",
    "#split the data to training set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## count all words and to find top 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count all words and find the top 2500 words\n",
    "data = ''\n",
    "for i in X_train:\n",
    "    data = data + \" \" + i\n",
    "cleandata =re.sub('[^A-Za-z]+', ' ', data.lower())\n",
    "datalist = cleandata.split()\n",
    "fdist2 = nltk.FreqDist(datalist)\n",
    "words = fdist2.most_common(2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split each review to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = list(X_train.values.flatten())\n",
    "y_train = list(y_train.values.flatten())\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i] = re.sub('[^A-Za-z]+', ' ', X_train[i].lower()).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define a function to generate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate feature matrix, for each reviews if the word in the review then label it to 1\n",
    "def generateX(X_re):\n",
    "    result = np.zeros(len(words))\n",
    "    for l in X_re:\n",
    "        x1 = np.zeros(len(words))\n",
    "        for i in range(len(words)):\n",
    "            if words[i][0] in l:\n",
    "                x1[i] = 1\n",
    "        result = np.column_stack((result, x1))\n",
    "    return result.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate features from X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features we generate from the first review of X_train:\n",
      "length: 2500\n",
      "[1. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "train_X = generateX(X_train)\n",
    "#delet the first row, since the first row is added by generateX function(which is all 0)\n",
    "train_X = np.delete(train_X,0,0)\n",
    "print('The features we generate from the first review of X_train:')\n",
    "print('length:',len(train_X[0]))\n",
    "print(train_X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## since MaxEnt is the same model as logistic regression, we use skleaner to train our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use the feature matrix we get from X_train to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv=5, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "           multi_class='multinomial', n_jobs=1, penalty='l2',\n",
       "           random_state=0, refit=True, scoring=None, solver='lbfgs',\n",
       "           tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training our model, the MaxEnt is same as Multinomial logistic regression model.\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "#training our model with cross-validation = 5\n",
    "clf = LogisticRegressionCV(cv=5, random_state=0, multi_class='multinomial')\n",
    "clf.fit(train_X, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate features from X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare our test dataset\n",
    "X_test = list(X_test.values.flatten())\n",
    "y_test = list(y_test.values.flatten())\n",
    "for i in range(len(X_test)):\n",
    "    X_test[i] = re.sub('[^A-Za-z]+', ' ', X_test[i].lower()).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = generateX(X_test)\n",
    "test_X = np.delete(test_X,0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put the feature matrix which from X_test in our classifier to do prediction, and print the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.8313333333333334\n"
     ]
    }
   ],
   "source": [
    "#test our model and out the accuracy\n",
    "final = clf.predict(test_X)\n",
    "print('Accuracy:')\n",
    "print(clf.score(test_X,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some words can affect the accuracy, such as 'is, the , are' and so on. Those words appear in both positive reviews and negative reviews. And those world must appear in top 2500 words list. This will affect accuracy. For improvment, we can find top 2500 words without those words to train our model. And also we can increase the size of words list, that also may improve the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ('Hot', 'Cold')\n",
    "end_state = 'End'\n",
    " \n",
    "observations1 = ('3','3','1','1','2','2','3','1','3')\n",
    "observations2 = ('3','3','1','1','2','2','3','1','2')\n",
    "\n",
    "start_probability = {'Hot': 0.8, 'Cold': 0.2}\n",
    "\n",
    "transition_probability = {\n",
    "   'Hot'  : {'Hot': 0.6, 'Cold': 0.3, 'End': 0.1},\n",
    "   'Cold' : {'Hot': 0.4, 'Cold': 0.5, 'End': 0.1},\n",
    "   }\n",
    " \n",
    "emission_probability = {\n",
    "   'Hot'  : {'1': 0.2, '2': 0.4, '3': 0.4},\n",
    "   'Cold' : {'1': 0.5, '2': 0.4, '3': 0.1},\n",
    "   }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foward & Backard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fwd_bkw function include both Forward Algorithm and Backward Algorithm\n",
    "def fwd_bkw(y, states, a_0, a, e, end_st):\n",
    "    L = len(y)\n",
    "    fwd = []\n",
    "    f_prev = {}\n",
    "    # forward part of the algorithm\n",
    "    for i, y_i in enumerate(y):\n",
    "        f_curr = {}\n",
    "        for st in states:\n",
    "            if i == 0:\n",
    "                # base case for the forward part\n",
    "                prev_f_sum = a_0[st]\n",
    "            else:\n",
    "                prev_f_sum = sum(f_prev[k]*a[k][st] for k in states)\n",
    " \n",
    "            f_curr[st] = e[st][y_i] * prev_f_sum\n",
    " \n",
    "        fwd.append(f_curr)\n",
    "        f_prev = f_curr\n",
    " \n",
    "    p_fwd=sum(f_curr[k]*a[k][end_st] for k in states)\n",
    "    p_fwd_list = fwd\n",
    "\n",
    "    bkw = []\n",
    "    b_prev = {}\n",
    "    # backward part of the algorithm\n",
    "    for i, y_i_plus in enumerate(reversed(y[1:]+(None,))):\n",
    "        b_curr = {}\n",
    "        for st in states:\n",
    "            if i == 0:\n",
    "                # base case for backward part\n",
    "                b_curr[st] = a[st][end_st]\n",
    "            else:\n",
    "                b_curr[st] = sum(a[st][l]*e[l][y_i_plus]*b_prev[l] for l in states)\n",
    " \n",
    "        bkw.insert(0,b_curr)\n",
    "        b_prev = b_curr\n",
    " \n",
    "    p_bkw = sum(a_0[l] * e[l][y[0]] * b_curr[l] for l in states)\n",
    "    bkw_list= bkw\n",
    "    # merging the two parts\n",
    "    posterior = []\n",
    "    for i in range(L):\n",
    "        posterior.append({st: fwd[i][st]*bkw[i][st]/p_fwd for st in states})\n",
    "        \n",
    " \n",
    "    \n",
    "    return p_fwd, p_bkw, p_fwd_list, bkw_list, posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example(observations):\n",
    "    return fwd_bkw(observations,\n",
    "                   states,\n",
    "                   start_probability,\n",
    "                   transition_probability,\n",
    "                   emission_probability,\n",
    "                   end_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implement Forward Algorithm & Backward Algorithm for 2 obserations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foreword:\n",
      "[{'Hot': 0.32000000000000006, 'Cold': 0.020000000000000004}, {'Hot': 0.08000000000000002, 'Cold': 0.010600000000000002}, {'Hot': 0.010448000000000002, 'Cold': 0.014650000000000003}, {'Hot': 0.0024257600000000008, 'Cold': 0.005229700000000001}, {'Hot': 0.0014189344000000003, 'Cold': 0.0013370312000000003}, {'Hot': 0.0005544692480000001, 'Cold': 0.00043767836800000015}, {'Hot': 0.00020310115840000006, 'Cold': 3.851799584000002e-05}, {'Hot': 2.7453578675200012e-05, 'Cold': 4.009467272000001e-05}, {'Hot': 1.3004006517248004e-05, 'Cold': 2.828340996256001e-06}]\n",
      "Backword:\n",
      "[{'Hot': 4.71096613296e-06, 'Cold': 3.7862794401600007e-06}, {'Hot': 1.6938878256000002e-05, 'Cold': 2.1521178384000004e-05}, {'Hot': 5.5919044800000004e-05, 'Cold': 6.819061920000001e-05}, {'Hot': 0.00020839824, 'Cold': 0.00020607504000000004}, {'Hot': 0.000588564, 'Cold': 0.000559524}, {'Hot': 0.0017559, 'Cold': 0.0013929000000000003}, {'Hot': 0.00639, 'Cold': 0.007410000000000002}, {'Hot': 0.027, 'Cold': 0.021000000000000005}, {'Hot': 0.1, 'Cold': 0.1}]\n",
      "Posterior:\n",
      "[{'Hot': 0.9521703343495896, 'Cold': 0.047829665650410226}, {'Hot': 0.8559124029612006, 'Cold': 0.14408759703879928}, {'Hot': 0.3690180370106695, 'Cold': 0.6309819629893306}, {'Hot': 0.31929826845401144, 'Cold': 0.6807017315459887}, {'Hot': 0.5274857095508314, 'Cold': 0.47251429044916854}, {'Hot': 0.6149388470236562, 'Cold': 0.3850611529763439}, {'Hot': 0.819724555104064, 'Cold': 0.18027544489593608}, {'Hot': 0.46818491294368264, 'Cold': 0.5318150870563175}, {'Hot': 0.8213568143420549, 'Cold': 0.17864318565794507}]\n"
     ]
    }
   ],
   "source": [
    "fwd, bkw, fwd_list, bkw_list,posterior = example(observations1)\n",
    "print('Foreword:')\n",
    "print(fwd_list)\n",
    "print('Backword:')\n",
    "print(bkw_list)\n",
    "print('Posterior:')\n",
    "print(posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foreword:\n",
      "[{'Hot': 0.32000000000000006, 'Cold': 0.020000000000000004}, {'Hot': 0.08000000000000002, 'Cold': 0.010600000000000002}, {'Hot': 0.010448000000000002, 'Cold': 0.014650000000000003}, {'Hot': 0.0024257600000000008, 'Cold': 0.005229700000000001}, {'Hot': 0.0014189344000000003, 'Cold': 0.0013370312000000003}, {'Hot': 0.0005544692480000001, 'Cold': 0.00043767836800000015}, {'Hot': 0.00020310115840000006, 'Cold': 3.851799584000002e-05}, {'Hot': 2.7453578675200012e-05, 'Cold': 4.009467272000001e-05}, {'Hot': 1.3004006517248004e-05, 'Cold': 1.1313363985024004e-05}]\n",
      "Backword:\n",
      "[{'Hot': 7.2357114412800015e-06, 'Cold': 5.815469450880002e-06}, {'Hot': 2.6016873408000007e-05, 'Cold': 3.305539411200001e-05}, {'Hot': 8.588384640000002e-05, 'Cold': 0.00010473874560000003}, {'Hot': 0.00032000832, 'Cold': 0.00031655232000000013}, {'Hot': 0.0009033120000000001, 'Cold': 0.0008601120000000002}, {'Hot': 0.0026892, 'Cold': 0.0021492000000000004}, {'Hot': 0.00972, 'Cold': 0.011880000000000002}, {'Hot': 0.036000000000000004, 'Cold': 0.036000000000000004}, {'Hot': 0.1, 'Cold': 0.1}]\n",
      "Posterior:\n",
      "[{'Hot': 0.9521702443088027, 'Cold': 0.04782975569119741}, {'Hot': 0.8559107459606036, 'Cold': 0.14408925403939662}, {'Hot': 0.3690014210637466, 'Cold': 0.6309985789362536}, {'Hot': 0.3192217605314986, 'Cold': 0.6807782394685017}, {'Hot': 0.5270884327781433, 'Cold': 0.4729115672218568}, {'Hot': 0.613174315694325, 'Cold': 0.386825684305675}, {'Hot': 0.8118243127740942, 'Cold': 0.1881756872259057}, {'Hot': 0.40642915409577673, 'Cold': 0.5935708459042235}, {'Hot': 0.5347620342435059, 'Cold': 0.4652379657564941}]\n"
     ]
    }
   ],
   "source": [
    "fwd, bkw, fwd_list, bkw_list,posterior = example(observations2)\n",
    "print('Foreword:')\n",
    "print(fwd_list)\n",
    "print('Backword:')\n",
    "print(bkw_list)\n",
    "print('Posterior:')\n",
    "print(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi1:  7.644119040000002e-07\n",
      "path1:  ['Hot', 'Hot', 'Cold', 'Cold', 'Hot', 'Hot', 'Hot', 'Hot', 'Hot']\n",
      "[{'Hot': 0.32000000000000006, 'Cold': 0.020000000000000004}, {'Hot': 0.07680000000000002, 'Cold': 0.009600000000000003}, {'Hot': 0.009216000000000002, 'Cold': 0.011520000000000002}, {'Hot': 0.0011059200000000003, 'Cold': 0.0028800000000000006}, {'Hot': 0.00046080000000000014, 'Cold': 0.0005760000000000001}, {'Hot': 0.00011059200000000003, 'Cold': 0.00011520000000000004}, {'Hot': 2.6542080000000007e-05, 'Cold': 5.7600000000000024e-06}, {'Hot': 3.1850496000000008e-06, 'Cold': 3.981312000000001e-06}, {'Hot': 7.644119040000002e-07, 'Cold': 1.9906560000000005e-07}]\n",
      "\n",
      "Viterbi2:  7.962624000000002e-07\n",
      "path2:  ['Hot', 'Hot', 'Cold', 'Cold', 'Hot', 'Hot', 'Hot', 'Cold', 'Cold']\n",
      "[{'Hot': 0.32000000000000006, 'Cold': 0.020000000000000004}, {'Hot': 0.07680000000000002, 'Cold': 0.009600000000000003}, {'Hot': 0.009216000000000002, 'Cold': 0.011520000000000002}, {'Hot': 0.0011059200000000003, 'Cold': 0.0028800000000000006}, {'Hot': 0.00046080000000000014, 'Cold': 0.0005760000000000001}, {'Hot': 0.00011059200000000003, 'Cold': 0.00011520000000000004}, {'Hot': 2.6542080000000007e-05, 'Cold': 5.7600000000000024e-06}, {'Hot': 3.1850496000000008e-06, 'Cold': 3.981312000000001e-06}, {'Hot': 7.644119040000002e-07, 'Cold': 7.962624000000002e-07}]\n"
     ]
    }
   ],
   "source": [
    "def viterbi(obs, states, start_p, trans_p, emit_p):\n",
    "    V = [{}]\n",
    "    path = {}\n",
    " \n",
    "    # Initialize base cases (t == 0)\n",
    "    for y in states:\n",
    "        V[0][y] = start_p[y] * emit_p[y][obs[0]]\n",
    "        path[y] = [y]\n",
    " \n",
    "    # Run Viterbi for t > 0\n",
    "    for t in range(1, len(obs)):\n",
    "        V.append({})\n",
    "        newpath = {}\n",
    " \n",
    "        for y in states:\n",
    "            (prob, state) = max((V[t-1][y0] * trans_p[y0][y] * emit_p[y][obs[t]], y0) for y0 in states)\n",
    "            V[t][y] = prob\n",
    "            newpath[y] = path[state] + [y]\n",
    " \n",
    "        # Don't need to remember the old paths\n",
    "        path = newpath\n",
    "     \n",
    "    #print_dptable(V)\n",
    "    (prob, state) = max((V[t][y], y) for y in states)\n",
    "    return (prob, path[state],V)\n",
    "\n",
    "\n",
    "\n",
    "prob1, p_hidden1,v1 = viterbi(observations1,states,start_probability,transition_probability,emission_probability)\n",
    "prob2, p_hidden2,v2 = viterbi(observations2,states,start_probability,transition_probability,emission_probability)\n",
    "# assess accuracy of the HMM model\n",
    "\n",
    "print(\"Viterbi1: \",prob1)\n",
    "print(\"path1: \",p_hidden1)\n",
    "print(v1)\n",
    "print()\n",
    "print(\"Viterbi2: \",prob2)\n",
    "print(\"path2: \",p_hidden2)\n",
    "print(v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.book import *\n",
    "import pprint\n",
    "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findlongest(text):\n",
    "    #since we don't know how to convert the text object of nltk to string, we simply add all tokens togather to form a string.\n",
    "    a = text.tokens\n",
    "    text = ''\n",
    "    for i in a:\n",
    "        if i.isalpha():\n",
    "            text = text + ' '+i\n",
    "       \n",
    "        else:\n",
    "            text = text +i\n",
    "    sents = sent_tokenizer.tokenize(text)\n",
    "    lensent = []\n",
    "    for i in sents:\n",
    "        lensent.append(len(i.split()))\n",
    "    longest = sents[lensent.index(max(lensent))]\n",
    "    return longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moby Dick - the longest sentence:\n",
      "Though in many natural objects, whiteness refiningly enhances beauty, as if imparting some special virtue of its own, as in marbles, japonicas, and pearls; and though various nations have in some way recognised a certain royal preeminence in this hue; even the barbaric, grand old kings of Pegu placing the title\" Lord of the White Elephants\" above all their other magniloquent ascriptions of dominion; and the modern kings of Siam unfurling the same snow- white quadruped in the royal standard; and the Hanoverian flag bearing the one figure of a snow- white charger; and the great Austrian Empire, Caesarian, heir to overlording Rome, having for the imperial colour the same imperial hue; and though this pre- eminence in it applies to the human race itself, giving the white man ideal mastership over every dusky tribe; and though, besides, all this, whiteness has been even made significant of gladness, for among the Romans a white stone marked a joyful day; and though in other mortal sympathies and symbolizings, this same hue is made the emblem of many touching, noble things-- the innocence of brides, the benignity of age; though among the Red Men of America the giving of the white belt of wampum was the deepest pledge of honour; though in many climes, whiteness typifies the majesty of Justice in the ermine of the Judge, and contributes to the daily state of kings and queens drawn by milk- white steeds; though even in the higher mysteries of the most august religions it has been made the symbol of the divine spotlessness and power; by the Persian fire worshippers, the white forked flame being held the holiest on the altar; and in the Greek mythologies, Great Jove himself being made incarnate in a snow- white bull; and though to the noble Iroquois, the midwinter sacrifice of the sacred White Dog was by far the holiest festival of their theology, that spotless, faithful creature being held the purest envoy they could send to the Great Spirit with the annual tidings of their own fidelity; and though directly from the Latin word for white, all Christian priests derive the name of one part of their sacred vesture, the alb or tunic, worn beneath the cassock; and though among the holy pomps of the Romish faith, white is specially employed in the celebration of the Passion of our Lord; though in the Vision of St. John, white robes are given to the redeemed, and the four- and- twenty elders stand clothed in white before the great- white throne, and the Holy One that sitteth there white like wool; yet for all these accumulated associations, with whatever is sweet, and honourable, and sublime, there yet lurks an elusive something in the innermost idea of this hue, which strikes more of panic to the soul than that redness which affrights in blood.\n"
     ]
    }
   ],
   "source": [
    "print('Moby Dick - the longest sentence:')\n",
    "print(findlongest(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sense and Sensibility - the longest sentence:\n",
      "I am sure you will be glad to hear, as likewise dear Mrs. Jennings, I spent two happy hours with him yesterday afternoon, he would not hear of our parting, though earnestly did I, as I thought my duty required, urge him to it for prudence sake, and would have parted for ever on the spot, would he consent to it; but he said it should never be, he did not regard his mother' s anger, while he could have my affections; our prospects are not very bright, to be sure, but we must wait, and hope for the best; he will be ordained shortly; and should it ever be in your power to recommend him to any body that has a living to bestow, am very sure you will not forget us, and dear Mrs. Jennings too, trust she will speak a good word for us to Sir John, or Mr. Palmer, or any friend that may be able to assist us.-- Poor Anne was much to blame for what she did, but she did it for the best, so I say nothing; hope Mrs. Jennings won' t think it too much trouble to give us a call, should she come this way any morning,' twould be a great kindness, and my cousins would be proud to know her.-- My paper reminds me to conclude; and begging to be most gratefully and respectfully remembered to her, and to Sir John, and Lady Middleton, and the dear children, when you chance to see them, and love to Miss Marianne,\" I am,& c.\" As soon as Elinor had finished it, she performed what she concluded to be its writer' s real design, by placing it in the hands of Mrs. Jennings, who read it aloud with many comments of satisfaction and praise.\"\n"
     ]
    }
   ],
   "source": [
    "print('Sense and Sensibility - the longest sentence:')\n",
    "print(findlongest(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inaugural Address Corpus - the longest sentence:\n",
      "On this subject it might become me better to be silent or to speak with diffidence; but as something may be expected, the occasion, I hope, will be admitted as an apology if I venture to say that if a preference, upon principle, of a free republican government, formed upon long and serious reflection, after a diligent and impartial inquiry after truth; if an attachment to the Constitution of the United States, and a conscientious determination to support it until it shall be altered by the judgments and wishes of the people, expressed in the mode prescribed in it; if a respectful attention to the constitutions of the individual States and a constant caution and delicacy toward the State governments; if an equal and impartial regard to the rights, interest, honor, and happiness of all the States in the Union, without preference or regard to a northern or southern, an eastern or western, position, their various political opinions on unessential points or their personal attachments; if a love of virtuous men of all parties and denominations; if a love of science and letters and a wish to patronize every rational effort to encourage schools, colleges, universities, academies, and every institution for propagating knowledge, virtue, and religion among all classes of the people, not only for their benign influence on the happiness of life in all its stages and classes, and of society in all its forms, but as the only means of preserving our Constitution from its natural enemies, the spirit of sophistry, the spirit of party, the spirit of intrigue, the profligacy of corruption, and the pestilence of foreign influence, which is the angel of destruction to elective governments; if a love of equal laws, of justice, and humanity in the interior administration; if an inclination to improve agriculture, commerce, and manufacturers for necessity, convenience, and defense; if a spirit of equity and humanity toward the aboriginal nations of America, and a disposition to meliorate their condition by inclining them to be more friendly to us, and our citizens to be more friendly to them; if an inflexible determination to maintain peace and inviolable faith with all nations, and that system of neutrality and impartiality among the belligerent powers of Europe which has been adopted by this Government and so solemnly sanctioned by both Houses of Congress and applauded by the legislatures of the States and the public opinion, until it shall be otherwise ordained by Congress; if a personal esteem for the French nation, formed in a residence of seven years chiefly among them, and a sincere desire to preserve the friendship which has been so much for the honor and interest of both nations; if, while the conscious honor and integrity of the people of America and the internal sentiment of their own power and energies must be preserved, an earnest endeavor to investigate every just cause and remove every colorable pretense of complaint; if an intention to pursue by amicable negotiation a reparation for the injuries that have been committed on the commerce of our fellow- citizens by whatever nation, and if success can not be obtained, to lay the facts before the Legislature, that they may consider what further measures the honor and interest of the Government and its constituents demand; if a resolution to do justice as far as may depend upon me, at all times and to all nations, and maintain peace, friendship, and benevolence with all the world; if an unshaken confidence in the honor, spirit, and resources of the American people, on which I have so often hazarded my all and never been deceived; if elevated ideas of the high destinies of this country and of my own duties toward it, founded on a knowledge of the moral principles and intellectual improvements of the people deeply engraven on my mind in early life, and not obscured but exalted by experience and age; and, with humble reverence, I feel it to be my duty to add, if a veneration for the religion of a people who profess and call themselves Christians, and a fixed resolution to consider a decent respect for Christianity among the best recommendations for the public service, can enable me in any degree to comply with your wishes, it shall be my strenuous endeavor that this sagacious injunction of the two Houses shall not be without effect.\n"
     ]
    }
   ],
   "source": [
    "print('Inaugural Address Corpus - the longest sentence:')\n",
    "print(findlongest(text4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall Street Journal - the longest sentence:\n",
      "But other than the fact that besuboru is played*-1 with a ball and a bat, it's unrecognizable: Fans politely return foul balls to stadium ushers; the strike zone expands depending on the size of the hitter; ties are permitted*-2-- even welcomed*-2-- since they honorably sidestep the shame of defeat; players must abide by strict rules of conduct even in their personal lives-- players for the Tokyo Giants, for example, must always wear ties when on the road.`` You Gotta Have Wa'' is the often amusing chronicle of how American ballplayers, rationed* to two per team, fare*T*-1 in Japan.\n"
     ]
    }
   ],
   "source": [
    "print('Wall Street Journal - the longest sentence:')\n",
    "print(findlongest(text7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Man Who Was Thursday - the longest sentence:\n",
      "As I say,\" resumed the Professor, like a man toiling through heavy sand,\" the incident that has occurred to us and has led us to ask for information about the Marquis, is one which you may think it better to have narrated; but as it came in the way of Comrade Syme rather than me--\" His words he seemed to be dragging out like words in an anthem; but Syme, who was watching, saw his long fingers rattle quickly on the edge of the crazy table.\n"
     ]
    }
   ],
   "source": [
    "print('The Man Who Was Thursday - the longest sentence:')\n",
    "print(findlongest(text9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are the longest sentence we found for those texts by using sent_tokenizer. The first letter of all of the sentences we found are capital letter. This means it only check the beginning sentence of every paragraphs and to find the longest one. And this make some mistakes on some text, like text2:Sense and Sensibility, the longest sentence it found is a paragraph(it may be the mistake we made)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
